{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cfec09-e874-41ee-974f-8e80e5e1e66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iceberg/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a031646-19be-43e5-9824-7aff402cdfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:10:39] [Thread-3] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-iceberg-minio-dev\")\n",
    "\n",
    "    # Iceberg catalog (HadoopCatalog on MinIO)\n",
    "    .config(\"spark.sql.catalog.local_minio\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local_minio.catalog-impl\", \"org.apache.iceberg.hadoop.HadoopCatalog\")\n",
    "    .config(\"spark.sql.catalog.local_minio.warehouse\", \"s3a://warehouse/\")\n",
    "\n",
    "    # Iceberg SQL extensions (mandatory)\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce82c83f-83d2-4430-b842-6a9e48293857",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"minioadmin\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"minioadmin\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Good defaults\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\",\n",
    "                 \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4847f090-3ec4-4035-aaf0-2c62ea911443",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36b163d-bada-457d-8133-738f00ef69a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04bc4d3-2570-446a-a8b8-ba6dc7ec332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4d6573-2612-428b-9684-57d5d78e6cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:28] [Thread-3] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(\"s3a://warehouse/\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16791dca-1be2-4fb9-b63e-eb52e81be682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:35] [Thread-3] INFO  org.apache.iceberg.CatalogUtil - Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "[02:11:35] [Thread-3] INFO  org.apache.iceberg.BaseMetastoreCatalog - Table properties set at catalog level through catalog properties: {}\n",
      "[02:11:35] [Thread-3] INFO  org.apache.iceberg.BaseMetastoreCatalog - Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:40] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkWrite - Committing append with 3 new data files to table test_db.test_table\n",
      "[02:11:40] [Thread-3] INFO  org.apache.iceberg.SnapshotProducer - Committed snapshot 2658278754181055859 (MergeAppend)\n",
      "[02:11:40] [Thread-3] INFO  org.apache.iceberg.metrics.LoggingMetricsReporter - Received metrics report: CommitReport{tableName=test_db.test_table, snapshotId=2658278754181055859, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.337192287S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=3}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1983}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1983}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=4.0.1, app-id=local-1767060640228, engine-name=spark, iceberg-version=Apache Iceberg 1.10.1 (commit ccb8bc435062171e64bc8b7e5f56e6aed9c5b934)}}\n",
      "[02:11:40] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkWrite - Committed in 361 ms\n",
      "[02:11:40] [Thread-3] INFO  org.apache.iceberg.hadoop.HadoopTableOperations - Committed a new metadata file s3a://warehouse/test_db/test_table/metadata/v1.metadata.json\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, \"a\"), (2, \"b\"), (3, \"c\")],\n",
    "    [\"id\", \"value\"]\n",
    ")\n",
    "\n",
    "df.writeTo(\"local_minio.test_db.test_table\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca4b19f-1e32-49ff-a65b-c76b156aeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:47] [Thread-3] INFO  org.apache.iceberg.BaseMetastoreCatalog - Table loaded by catalog: local_minio.test_db.test_table\n",
      "[02:11:47] [Thread-3] INFO  org.apache.iceberg.SnapshotScan - Scanning table local_minio.test_db.test_table snapshot 2658278754181055859 created at 2025-12-30T02:11:40.245+00:00 with filter true\n",
      "[02:11:47] [Thread-3] INFO  org.apache.iceberg.BaseDistributedDataScan - Planning file tasks locally for table local_minio.test_db.test_table\n",
      "[02:11:47] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkPartitioningAwareScan - Reporting UnknownPartitioning with 1 partition(s) for table local_minio.test_db.test_table\n",
      "[02:11:47] [Executor task launch worker for task 0.0 in stage 1.0 (TID 12)] INFO  org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders - Enabling arrow.enable_unsafe_memory_access\n",
      "[02:11:47] [Executor task launch worker for task 0.0 in stage 1.0 (TID 12)] INFO  org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders - Disabling arrow.enable_null_check_for_get\n",
      "[02:11:47] [Executor task launch worker for task 0.0 in stage 1.0 (TID 12)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.BaseAllocator - Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.\n",
      "[02:11:47] [Executor task launch worker for task 0.0 in stage 1.0 (TID 12)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.DefaultAllocationManagerOption - allocation manager type not specified, using netty as the default type\n",
      "[02:11:47] [Executor task launch worker for task 0.0 in stage 1.0 (TID 12)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.CheckAllocator - Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    a|\n",
      "|  2|    b|\n",
      "|  3|    c|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local_minio.test_db.test_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85003143-eb66-4b2a-ac6a-fd31079acb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
