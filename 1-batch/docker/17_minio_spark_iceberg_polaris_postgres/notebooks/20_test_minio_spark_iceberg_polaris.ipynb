{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a17eca-e587-4884-889d-1430b2f0c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_CONF_DIR = /opt/spark/conf\n",
      "SPARK_HOME = /opt/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"SPARK_CONF_DIR =\", os.environ.get(\"SPARK_CONF_DIR\"))\n",
    "print(\"SPARK_HOME =\", os.environ.get(\"SPARK_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a27482-3b75-4954-9803-fdddcf85da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 lines of spark-defaults.conf: ['# ==============================================================================\\n', '# SPARK CONFIGURATION FOR POLARIS + ICEBERG + MINIO\\n', '# ==============================================================================\\n', '\\n', '# Basic Extensions\\n', '# https://iceberg.apache.org/docs/latest/spark-configuration\\n', 'spark.sql.extensions                   org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\\n', '\\n', '# Catalog Configuration (The Polaris Connection)\\n', '# https://iceberg.apache.org/docs/latest/spark-configuration\\n']\n"
     ]
    }
   ],
   "source": [
    "conf_path = \"/opt/spark/conf/spark-defaults.conf\"\n",
    "with open(conf_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(\"First 10 lines of spark-defaults.conf:\", lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500d45a6-76bd-416f-8ee7-f01bada6df99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:49:42] [Thread-3] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Polaris Debug\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328632a0-9e4f-4a61-8bb2-fac1880db7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://polaris:8181/api/catalog\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(spark.conf.get(\"spark.sql.catalog.polaris.uri\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b002e011-9d03-4952-b7f2-2a5b2c6d8270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:49:51] [Thread-3] INFO  org.apache.iceberg.rest.auth.AuthManagers - Loading AuthManager implementation: org.apache.iceberg.rest.auth.OAuth2Manager\n",
      "[07:49:53] [Thread-3] INFO  org.apache.iceberg.CatalogUtil - Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS polaris.olist_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698728b4-542e-45d9-ba27-8ef2bcad89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|olist_raw|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN polaris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff023746-ca10-45e7-b147-e065ef9780bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:49:59] [Thread-3] INFO  org.apache.iceberg.CatalogUtil - Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS polaris.olist_raw.test_connection (\n",
    "        id INT,\n",
    "        status STRING\n",
    "    ) USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b0f84e-dca3-4bea-981f-ee2431c991a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'172.18.0.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "socket.gethostbyname(\"minio\")  # should return the internal IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33492b0-8a38-4eb8-b620-c6942ff89375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:50:05] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkWrite - Committing append with 2 new data files to table polaris.olist_raw.test_connection\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.SnapshotProducer - Committed snapshot 4322786829089748504 (MergeAppend)\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.metrics.LoggingMetricsReporter - Received metrics report: CommitReport{tableName=polaris.olist_raw.test_connection, snapshotId=4322786829089748504, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.705782153S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=4}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=4}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1327}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2654}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=4.0.1, app-id=local-1767426583712, engine-name=spark, iceberg-version=Apache Iceberg 1.10.1 (commit ccb8bc435062171e64bc8b7e5f56e6aed9c5b934)}}\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkWrite - Committed in 849 ms\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.SnapshotScan - Scanning table polaris.olist_raw.test_connection snapshot 4322786829089748504 created at 2026-01-03T07:50:06.218+00:00 with filter true\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.BaseDistributedDataScan - Planning file tasks locally for table polaris.olist_raw.test_connection\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.metrics.LoggingMetricsReporter - Received metrics report: ScanReport{tableName=polaris.olist_raw.test_connection, snapshotId=4322786829089748504, filter=true, schemaId=0, projectedFieldIds=[1, 2], projectedFieldNames=[id, status], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.08970833S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=4}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=2}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=2}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=2654}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}, dvs=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=4.0.1, iceberg-version=Apache Iceberg 1.10.1 (commit ccb8bc435062171e64bc8b7e5f56e6aed9c5b934), app-id=local-1767426583712, engine-name=spark}}\n",
      "[07:50:06] [Thread-3] INFO  org.apache.iceberg.spark.source.SparkPartitioningAwareScan - Reporting UnknownPartitioning with 1 partition(s) for table polaris.olist_raw.test_connection\n",
      "[07:50:07] [Executor task launch worker for task 0.0 in stage 1.0 (TID 2)] INFO  org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders - Enabling arrow.enable_unsafe_memory_access\n",
      "[07:50:07] [Executor task launch worker for task 0.0 in stage 1.0 (TID 2)] INFO  org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders - Disabling arrow.enable_null_check_for_get\n",
      "[07:50:07] [Executor task launch worker for task 0.0 in stage 1.0 (TID 2)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.BaseAllocator - Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.\n",
      "[07:50:07] [Executor task launch worker for task 0.0 in stage 1.0 (TID 2)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.DefaultAllocationManagerOption - allocation manager type not specified, using netty as the default type\n",
      "[07:50:07] [Executor task launch worker for task 0.0 in stage 1.0 (TID 2)] INFO  org.apache.iceberg.shaded.org.apache.arrow.memory.CheckAllocator - Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class\n",
      "+---+--------+\n",
      "| id|  status|\n",
      "+---+--------+\n",
      "|  1| pending|\n",
      "|  1| pending|\n",
      "|  2|complete|\n",
      "|  2|complete|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO polaris.olist_raw.test_connection VALUES\n",
    "(1, 'pending'),\n",
    "(2, 'complete')\n",
    "\"\"\")\n",
    "\n",
    "# Read back\n",
    "spark.sql(\"SELECT * FROM polaris.olist_raw.test_connection\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8f7ac6-c551-437f-9708-2040d2db3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FOUND: ('spark.sql.catalog.polaris.s3.endpoint', 'http://minio:9000/')\n"
     ]
    }
   ],
   "source": [
    "# Check what Spark actually loaded\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "for item in conf:\n",
    "    if \"s3.endpoint\" in item[0]:\n",
    "        print(f\"✅ FOUND: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6054931b-5bed-43b3-8b58-51418471f922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '18872923B65F34B5', 'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'accept-ranges': 'bytes', 'content-length': '543', 'content-type': 'application/xml', 'server': 'MinIO', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'vary': 'Origin, Accept-Encoding', 'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8', 'x-amz-request-id': '18872923B65F34B5', 'x-content-type-options': 'nosniff', 'x-ratelimit-limit': '2051', 'x-ratelimit-remaining': '2051', 'x-xss-protection': '1; mode=block', 'date': 'Sat, 03 Jan 2026 07:50:12 GMT'}, 'RetryAttempts': 0}, 'Buckets': [{'Name': 'raw', 'CreationDate': datetime.datetime(2026, 1, 3, 7, 43, 40, 415000, tzinfo=tzlocal())}, {'Name': 'staging', 'CreationDate': datetime.datetime(2026, 1, 3, 7, 43, 40, 447000, tzinfo=tzlocal())}, {'Name': 'warehouse', 'CreationDate': datetime.datetime(2026, 1, 3, 7, 43, 40, 378000, tzinfo=tzlocal())}], 'Owner': {'DisplayName': 'minio', 'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(s3.list_buckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c3cf1-fd08-4713-9eef-ed2ca4c6ac5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
